model   MeshGraphNets
mode    Train  # Train / Inference
gpu_ids 0      # -1 for CPU, GPU ids for multi-GPU training
log_file_dir    train_flag_dynamic1.log
%   Common params
input_var   4   # number of input variables: x_disp, y_disp, z_disp, stress (excluding node types)
output_var  4   # number of output variables: x_disp, y_disp, z_disp, stress (excluding node types)
edge_var    4   # dx, dy, dz, disp
'
%   Network parameters
dataset_dir ./dataset/flag_dynamic.h5
message_passing_num 15
Training_epochs	500
Batch_size	1
LearningR	0.0001
Latent_dim	256	# MeshGraphNets latent dimension
num_workers 10
std_noise   0.0001
residual_scale  0.1  # Scale factor for residual connections (0.1 = 10% of update added to current state)
verbose     False
monitor_gradients  False
'
% Memory Optimization
use_checkpointing   False
'
% Performance Optimization
use_parallel_stats  True    # Enable parallel processing for computing dataset statistics (speeds up initialization for large datasets, requires >=100 samples)
'
% Node Type Parameters
use_node_types  True    # Add one-hot encoded node types to node features
'
% World Edge Parameters
use_world_edges         True
world_radius_multiplier 1.5     # r_world = multiplier * min_mesh_edge_length (auto-computed)
world_max_num_neighbors 64      # Max neighbors per node in world edge radius query (prevents edge explosion)
world_edge_backend      scipy_kdtree   # Backend: torch_cluster (GPU, fast) or scipy_kdtree (CPU, fallback)
% Test set control
display_testset True
test_batch_idx  0, 1, 2, 3
plot_feature_idx    -2  # Feature index to visualize in plots (-1 = last feature, i.e., stress)